{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements for WRMSSE metric:\n",
    "* Function to convert level 12 series predictions to all level predictions by aggregating\n",
    "* Actual labels for all levels\n",
    "* Series ID as part of each DataLoader output\n",
    "* Function to create a dictionary with weights for all levels acc to series ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "from tqdm import notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import pickle as pkl\n",
    "from itertools import product\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.from_numpy(np.array([[1, 2, 3], [1, 5, 6]]))\n",
    "b = torch.from_numpy(np.array([10, 100]))\n",
    "c = torch.zeros(7, 3, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0],\n",
       "        [110, 110, 110],\n",
       "        [ 10,  10,  10],\n",
       "        [ 10,  10,  10],\n",
       "        [  0,   0,   0],\n",
       "        [100, 100, 100],\n",
       "        [100, 100, 100]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.index_add(0, a.flatten(), b.repeat_interleave(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10, 100])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[a.flatten()] = c[a.flatten()] + b.repeat_interleave(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 100,  10,  10,   0, 100, 100])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../../data/sales_train_validation.csv')\n",
    "sell_prices = pd.read_csv('../../data/sell_prices.csv')\n",
    "calendar = pd.read_csv('../../data/calendar.csv')\n",
    "sample_submission = pd.read_csv('../../data/sample_submission.csv')\n",
    "weights_validation = pd.read_csv('../../data/weights_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data.pickle', 'rb') as f:\n",
    "    data_dict = pkl.load(f)\n",
    "    \n",
    "sales_data_ids = data_dict['sales_data_ids']\n",
    "calendar_index = data_dict['calendar_index']\n",
    "X_prev_day_sales = data_dict['X_prev_day_sales']\n",
    "X_enc_only_feats = data_dict['X_enc_only_feats']\n",
    "X_enc_dec_feats = data_dict['X_enc_dec_feats']\n",
    "X_calendar = data_dict['X_calendar']\n",
    "X_calendar_cols = data_dict['X_calendar_cols']\n",
    "Y = data_dict['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all aggregated series from level 12 series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregated_series(sales, sales_data_ids):\n",
    "    \"\"\"\n",
    "    Aggregates 30,490 level 12 series to generate data for all 42,840 series\n",
    "    \n",
    "    Input data format:\n",
    "    sales: np array of shape (30490, num_timesteps)\n",
    "    sales_data_ids: np array of shape (30490, 5) \n",
    "                    with 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id' as the columns\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame({col: sales_data_ids[:, i] for col, i in \n",
    "                       zip(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], range(0, 5))})\n",
    "    df = pd.concat([df, pd.DataFrame(sales)], axis=1)\n",
    "    data_cols = [i for i in range(0, sales.shape[1])]\n",
    "    \n",
    "    agg_indices, agg_series, agg_series_id = [], [], []\n",
    "    \n",
    "    # Level 1\n",
    "    agg_series.append(sales.sum(0).reshape(1, -1))\n",
    "    agg_series_id.append(np.array(['Level1_Total_X']))\n",
    "    \n",
    "    # Level 2\n",
    "    agg = df.groupby(['state_id'])[data_cols]\n",
    "    agg_indices.append(agg.indices)\n",
    "    agg = agg.sum()\n",
    "    agg_series.append(agg.values)\n",
    "    agg_series_id.append(('Level2_' + agg.index.values + '_X'))\n",
    "        \n",
    "    # Level 3\n",
    "    agg = df.groupby(['store_id'])[data_cols]\n",
    "    agg_indices.append(agg.indices)\n",
    "    agg = agg.sum()\n",
    "    agg_series.append(agg.values)\n",
    "    agg_series_id.append(('Level3_' + agg.index.values + '_X'))\n",
    "        \n",
    "    # Level 4\n",
    "    agg = df.groupby(['cat_id'])[data_cols]\n",
    "    agg_indices.append(agg.indices)\n",
    "    agg = agg.sum()\n",
    "    agg_series.append(agg.values)\n",
    "    agg_series_id.append(('Level4_' + agg.index.values + '_X'))\n",
    "        \n",
    "    # Level 5\n",
    "    agg = df.groupby(['dept_id'])[data_cols]\n",
    "    agg_indices.append(agg.indices)\n",
    "    agg = agg.sum()\n",
    "    agg_series.append(agg.values)\n",
    "    agg_series_id.append(('Level5_' + agg.index.values + '_X'))\n",
    "    \n",
    "    # Level 6\n",
    "    agg = df.groupby(['state_id', 'cat_id'])[data_cols]\n",
    "    agg_indices.append(agg.indices)\n",
    "    agg = agg.sum()\n",
    "    agg_series.append(agg.values)\n",
    "    agg_series_id.append('Level6_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
    "    \n",
    "    # Level 7\n",
    "    agg = df.groupby(['state_id', 'dept_id'])[data_cols]\n",
    "    agg_indices.append(agg.indices)\n",
    "    agg = agg.sum()\n",
    "    agg_series.append(agg.values)\n",
    "    agg_series_id.append('Level7_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
    "        \n",
    "    # Level 8\n",
    "    agg = df.groupby(['store_id', 'cat_id'])[data_cols]\n",
    "    agg_indices.append(agg.indices)\n",
    "    agg = agg.sum()\n",
    "    agg_series.append(agg.values)\n",
    "    agg_series_id.append('Level8_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
    "\n",
    "    # Level 9\n",
    "    agg = df.groupby(['store_id', 'dept_id'])[data_cols]\n",
    "    agg_indices.append(agg.indices)\n",
    "    agg = agg.sum()\n",
    "    agg_series.append(agg.values)\n",
    "    agg_series_id.append('Level9_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
    "\n",
    "    # Level 10\n",
    "    agg = df.groupby(['item_id'])[data_cols]\n",
    "    agg_indices.append(agg.indices)\n",
    "    agg = agg.sum()\n",
    "    agg_series.append(agg.values)\n",
    "    agg_series_id.append(('Level10_' + agg.index.values + '_X'))\n",
    "\n",
    "    # Level 11\n",
    "    agg = df.groupby(['state_id', 'item_id'])[data_cols]\n",
    "    agg_indices.append(agg.indices)\n",
    "    agg = agg.sum()\n",
    "    agg_series.append(agg.values)\n",
    "    agg_series_id.append('Level11_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
    "    \n",
    "    # Level 12\n",
    "    agg = df.set_index(['item_id', 'store_id'])[data_cols]\n",
    "    agg_series.append(agg.values)\n",
    "    agg_series_id.append('Level12_' + agg.index.get_level_values(0) + '_' + agg.index.get_level_values(1))\n",
    "    \n",
    "    # Get affected_hierarchy_ids - all the series affected on updating each Level 12 series\n",
    "    affected_hierarchy_ids = np.empty((30490, 12), np.int32)\n",
    "\n",
    "    # Level 1\n",
    "    affected_hierarchy_ids[:, 0] = 0\n",
    "    fill_id, fill_col = 1, 1\n",
    "    # Level 2\n",
    "    for k, v in agg_indices[0].items():\n",
    "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
    "        fill_id += 1\n",
    "    fill_col += 1\n",
    "    # Level 3\n",
    "    for k, v in agg_indices[1].items():\n",
    "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
    "        fill_id += 1\n",
    "    fill_col += 1\n",
    "    # Level 4\n",
    "    for k, v in agg_indices[2].items():\n",
    "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
    "        fill_id += 1\n",
    "    fill_col += 1\n",
    "    # Level 5\n",
    "    for k, v in agg_indices[3].items():\n",
    "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
    "        fill_id += 1\n",
    "    fill_col += 1\n",
    "    # Level 6\n",
    "    for k, v in agg_indices[4].items():\n",
    "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
    "        fill_id += 1\n",
    "    fill_col += 1\n",
    "    # Level 7\n",
    "    for k, v in agg_indices[5].items():\n",
    "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
    "        fill_id += 1\n",
    "    fill_col += 1\n",
    "    # Level 8\n",
    "    for k, v in agg_indices[6].items():\n",
    "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
    "        fill_id += 1\n",
    "    fill_col += 1\n",
    "    # Level 9\n",
    "    for k, v in agg_indices[7].items():\n",
    "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
    "        fill_id += 1\n",
    "    fill_col += 1\n",
    "    # Level 10\n",
    "    for k, v in agg_indices[8].items():\n",
    "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
    "        fill_id += 1\n",
    "    fill_col += 1\n",
    "    # Level 11\n",
    "    for k, v in agg_indices[9].items():\n",
    "        affected_hierarchy_ids[v, fill_col] = fill_id\n",
    "        fill_id += 1\n",
    "    fill_col += 1\n",
    "    # Level 12\n",
    "    affected_hierarchy_ids[:, fill_col] = fill_id + np.arange(0, 30490)\n",
    "\n",
    "    return np.concatenate(agg_series, axis=0), np.concatenate(agg_series_id, axis=0).astype('<U28'), affected_hierarchy_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_series, agg_series_id, aff_hier_ids = get_aggregated_series(Y, sales_data_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.99 s ± 59.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "agg_series, agg_series_id, _ = get_aggregated_series(Y, sales_data_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate weights for 42,840 series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_all_levels(sales, sell_price, sales_data_ids):\n",
    "    \"\"\"\n",
    "    Generates weights for all 42,840 series\n",
    "    \n",
    "    Input data format:\n",
    "    sales: np array of shape (30490, 28)\n",
    "    sell_price: np array of shape (30490, 28)\n",
    "    \n",
    "    sales_data_ids: np array of shape (30490, 5) \n",
    "                with 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id' as the columns\n",
    "    \"\"\"\n",
    "    \n",
    "    assert (sales.shape == sell_price.shape), \"Sell price and Sales arrays have different sizes\"\n",
    "    assert (sales.shape[1] == 28), \"Number of timesteps provided weight calculation is not equal to 28\"\n",
    "    \n",
    "    # Get actual dollar sales for last 28 days for all 42,840 series\n",
    "    dollar_sales = sales * sell_price\n",
    "    agg_series, agg_series_id, _ = get_aggregated_series(dollar_sales, sales_data_ids)\n",
    "    \n",
    "    # Sum up the actual dollar sales for all 28 timesteps\n",
    "    agg_series = agg_series.sum(1)\n",
    "    \n",
    "    # Calculate total sales for each level\n",
    "    level_totals = agg_series[np.core.defchararray.find(agg_series_id, f'Level1_') == 0].sum()\n",
    "    \n",
    "    # Calculate weight for each series\n",
    "    weights = agg_series / level_totals\n",
    "    \n",
    "    return weights, agg_series_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, agg_series_id = get_weights_all_levels(Y[:, -84:-56], X_enc_dec_feats[:, :, 0].T[:, -84:-56], \n",
    "                                 sales_data_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_validation = pd.read_csv('../../data/weights_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.DataFrame({'id': agg_series_id, 'my_weight': weights})\n",
    "\n",
    "weights_validation['id'] = weights_validation['Level_id'] + '_' \\\n",
    "                            + weights_validation['Agg_Level_1'] + '_' + weights_validation['Agg_Level_2']\n",
    "weights_validation = weights_validation.merge(right=weights, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(((weights_validation.Weight - weights_validation.my_weight).values < 1e-7) != True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
